{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Census_JSON_Serving_Input_Fn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "environment": {
      "name": "tf-gpu.1-15.m55",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m55"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTMRv2OhAMEB",
        "colab_type": "text"
      },
      "source": [
        "## Install necessary packages if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F2uKbu7zGLnW",
        "colab": {}
      },
      "source": [
        "!pip install apache-beam==2.23.0\n",
        "!pip install tensorflow_transform==0.23.0\n",
        "!pip install --upgrade google-api-python-client=1.11.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWeGlYR9AMEG",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6TKRrIois11c",
        "colab": {}
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "48gEDke_Femr",
        "colab": {}
      },
      "source": [
        "# Copyright 2017 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Example using census data from UCI repository.\"\"\"\n",
        "\n",
        "# pylint: disable=g-bad-import-order\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "# GOOGLE-INITIALIZATION\n",
        "\n",
        "import apache_beam as beam\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "\n",
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital_status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'native_country',\n",
        "]\n",
        "NUMERIC_FEATURE_KEYS = [\n",
        "    'age',\n",
        "    'capital_gain',\n",
        "    'capital_loss',\n",
        "    'hours_per_week',\n",
        "    'education_num',\n",
        "]\n",
        "OPTIONAL_NUMERIC_FEATURE_KEYS = [\n",
        "#    'education-num',\n",
        "]\n",
        "LABEL_KEY = 'label'\n",
        "\n",
        "\n",
        "class MapAndFilterErrors(beam.PTransform):\n",
        "  \"\"\"Like beam.Map but filters out erros in the map_fn.\"\"\"\n",
        "\n",
        "  class _MapAndFilterErrorsDoFn(beam.DoFn):\n",
        "    \"\"\"Count the bad examples using a beam metric.\"\"\"\n",
        "\n",
        "    def __init__(self, fn):\n",
        "      self._fn = fn\n",
        "      # Create a counter to measure number of bad elements.\n",
        "      self._bad_elements_counter = beam.metrics.Metrics.counter(\n",
        "          'census_example', 'bad_elements')\n",
        "\n",
        "    def process(self, element):\n",
        "      try:\n",
        "        yield self._fn(element)\n",
        "      except Exception:  # pylint: disable=broad-except\n",
        "        # Catch any exception the above call.\n",
        "        self._bad_elements_counter.inc(1)\n",
        "\n",
        "  def __init__(self, fn):\n",
        "    self._fn = fn\n",
        "\n",
        "  def expand(self, pcoll):\n",
        "    return pcoll | beam.ParDo(self._MapAndFilterErrorsDoFn(self._fn))\n",
        "\n",
        "\n",
        "# Use this for the tf transform feature spec\n",
        "RAW_DATA_FEATURE_SPEC = dict([(name, tf.io.FixedLenFeature([], tf.string))\n",
        "                              for name in CATEGORICAL_FEATURE_KEYS] +\n",
        "                             [(name, tf.io.FixedLenFeature([], tf.float32))\n",
        "                              for name in NUMERIC_FEATURE_KEYS] +\n",
        "                             [(name, tf.io.VarLenFeature(tf.float32))\n",
        "                              for name in OPTIONAL_NUMERIC_FEATURE_KEYS] +\n",
        "                             [(LABEL_KEY,\n",
        "                               tf.io.FixedLenFeature([], tf.string))])\n",
        "\n",
        "# Use this for the input feature spec for json serving input fn\n",
        "# (Workaround for the get shape issue)\n",
        "RAW_DATA_FEATURE_SPEC_PH = dict([(name, tf.compat.v1.placeholder(shape=[None], dtype=tf.string))\n",
        "                              for name in CATEGORICAL_FEATURE_KEYS] +\n",
        "                             [(name, tf.compat.v1.placeholder(shape=[None], dtype=tf.float32))\n",
        "                              for name in NUMERIC_FEATURE_KEYS] +\n",
        "                             [(name, tf.compat.v1.placeholder(shape=[None], dtype=tf.float32))\n",
        "                              for name in OPTIONAL_NUMERIC_FEATURE_KEYS] +\n",
        "                             [(LABEL_KEY,\n",
        "                               tf.compat.v1.placeholder(shape=[None], dtype=tf.string))])\n",
        "\n",
        "RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
        "    schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC))\n",
        "\n",
        "# Constants used for training.  Note that the number of instances will be\n",
        "# computed by tf.Transform in future versions, in which case it can be read from\n",
        "# the metadata.  Similarly BUCKET_SIZES will not be needed as this information\n",
        "# will be stored in the metadata for each of the columns.  The bucket size\n",
        "# includes all listed categories in the dataset description as well as one extra\n",
        "# for \"?\" which represents unknown.\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "TRAIN_NUM_EPOCHS = 1\n",
        "NUM_TRAIN_INSTANCES = 32561\n",
        "NUM_TEST_INSTANCES = 16281\n",
        "\n",
        "# Names of temp files\n",
        "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
        "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
        "EXPORTED_MODEL_DIR = 'exported_model_dir'\n",
        "\n",
        "# Functions for preprocessing\n",
        "\n",
        "\n",
        "def transform_data(train_data_file, test_data_file, working_dir):\n",
        "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
        "\n",
        "  Read in the data using the CSV reader, and transform it using a\n",
        "  preprocessing pipeline that scales numeric data and converts categorical data\n",
        "  from strings to int64 values indices, by creating a vocabulary for each\n",
        "  category.\n",
        "\n",
        "  Args:\n",
        "    train_data_file: File containing training data\n",
        "    test_data_file: File containing test data\n",
        "    working_dir: Directory to write transformed data and metadata to\n",
        "  \"\"\"\n",
        "\n",
        "  def preprocessing_fn(inputs):\n",
        "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "    # Since we are modifying some features and leaving others unchanged, we\n",
        "    # start by setting `outputs` to a copy of `inputs.\n",
        "    outputs = inputs.copy()\n",
        "\n",
        "    # Scale numeric columns to have range [0, 1].\n",
        "    for key in NUMERIC_FEATURE_KEYS:\n",
        "      outputs[key] = tft.scale_to_0_1(outputs[key])\n",
        "\n",
        "    for key in OPTIONAL_NUMERIC_FEATURE_KEYS:\n",
        "      # This is a SparseTensor because it is optional. Here we fill in a default\n",
        "      # value when it is missing.\n",
        "      sparse = tf.sparse.SparseTensor(outputs[key].indices, outputs[key].values,\n",
        "                                      [outputs[key].dense_shape[0], 1])\n",
        "      dense = tf.sparse.to_dense(sp_input=sparse, default_value=0.)\n",
        "      # Reshaping from a batch of vectors of size 1 to a batch to scalars.\n",
        "      dense = tf.squeeze(dense, axis=1)\n",
        "      outputs[key] = tft.scale_to_0_1(dense)\n",
        "\n",
        "    # For all categorical columns except the label column, we generate a\n",
        "    # vocabulary but do not modify the feature.  This vocabulary is instead\n",
        "    # used in the trainer, by means of a feature column, to convert the feature\n",
        "    # from a string to an integer id.\n",
        "    for key in CATEGORICAL_FEATURE_KEYS:\n",
        "      tft.vocabulary(inputs[key], vocab_filename=key)\n",
        "\n",
        "    # For the label column we provide the mapping from string to index.\n",
        "    table_keys = ['>50K', '<=50K']\n",
        "    initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=table_keys,\n",
        "        values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
        "        key_dtype=tf.string,\n",
        "        value_dtype=tf.int64)\n",
        "    table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
        "    outputs[LABEL_KEY] = table.lookup(outputs[LABEL_KEY])\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
        "  # of the block.\n",
        "  with beam.Pipeline() as pipeline:\n",
        "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "      # Create a coder to read the census data with the schema.  To do this we\n",
        "      # need to list all columns in order since the schema doesn't specify the\n",
        "      # order of columns in the csv.\n",
        "      ordered_columns = [\n",
        "          'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "          'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
        "          'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "          'label'\n",
        "      ]\n",
        "      converter = tft.coders.CsvCoder(ordered_columns, RAW_DATA_METADATA.schema)\n",
        "\n",
        "      # Read in raw data and convert using CSV converter.  Note that we apply\n",
        "      # some Beam transformations here, which will not be encoded in the TF\n",
        "      # graph since we don't do the from within tf.Transform's methods\n",
        "      # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
        "      # to get data into a format that the CSV converter can read, in particular\n",
        "      # removing spaces after commas.\n",
        "      #\n",
        "      # We use MapAndFilterErrors instead of Map to filter out decode errors in\n",
        "      # convert.decode which should only occur for the trailing blank line.\n",
        "      raw_data = (\n",
        "          pipeline\n",
        "          | 'ReadTrainData' >> beam.io.ReadFromText(train_data_file)\n",
        "          | 'FixCommasTrainData' >> beam.Map(\n",
        "              lambda line: line.replace(', ', ','))\n",
        "          | 'DecodeTrainData' >> MapAndFilterErrors(converter.decode))\n",
        "\n",
        "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
        "      # the schema to read the CSV data, but we also need it to interpret\n",
        "      # raw_data.\n",
        "      raw_dataset = (raw_data, RAW_DATA_METADATA)\n",
        "      transformed_dataset, transform_fn = (\n",
        "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n",
        "      transformed_data, transformed_metadata = transformed_dataset\n",
        "      transformed_data_coder = tft.coders.ExampleProtoCoder(\n",
        "          transformed_metadata.schema)\n",
        "\n",
        "      _ = (\n",
        "          transformed_data\n",
        "          | 'EncodeTrainData' >> beam.Map(transformed_data_coder.encode)\n",
        "          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n",
        "\n",
        "      # Now apply transform function to test data.  In this case we remove the\n",
        "      # trailing period at the end of each line, and also ignore the header line\n",
        "      # that is present in the test data file.\n",
        "      raw_test_data = (\n",
        "          pipeline\n",
        "          | 'ReadTestData' >> beam.io.ReadFromText(test_data_file,\n",
        "                                                   skip_header_lines=1)\n",
        "          | 'FixCommasTestData' >> beam.Map(\n",
        "              lambda line: line.replace(', ', ','))\n",
        "          | 'RemoveTrailingPeriodsTestData' >> beam.Map(lambda line: line[:-1])\n",
        "          | 'DecodeTestData' >> MapAndFilterErrors(converter.decode))\n",
        "\n",
        "      raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n",
        "\n",
        "      transformed_test_dataset = (\n",
        "          (raw_test_dataset, transform_fn) | tft_beam.TransformDataset())\n",
        "      # Don't need transformed data schema, it's the same as before.\n",
        "      transformed_test_data, _ = transformed_test_dataset\n",
        "\n",
        "      _ = (\n",
        "          transformed_test_data\n",
        "          | 'EncodeTestData' >> beam.Map(transformed_data_coder.encode)\n",
        "          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n",
        "\n",
        "      # Will write a SavedModel and metadata to working_dir, which can then\n",
        "      # be read by the tft.TFTransformOutput class.\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))\n",
        "\n",
        "# Functions for training\n",
        "\n",
        "\n",
        "def _make_training_input_fn(tf_transform_output, transformed_examples,\n",
        "                            batch_size):\n",
        "  \"\"\"Creates an input function reading from transformed data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    transformed_examples: Base filename of examples.\n",
        "    batch_size: Batch size.\n",
        "\n",
        "  Returns:\n",
        "    The input function for training or eval.\n",
        "  \"\"\"\n",
        "  def input_fn():\n",
        "    \"\"\"Input function for training and eval.\"\"\"\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=transformed_examples,\n",
        "        batch_size=batch_size,\n",
        "        features=tf_transform_output.transformed_feature_spec(),\n",
        "        reader=tf.data.TFRecordDataset,\n",
        "        shuffle=True)\n",
        "\n",
        "    transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n",
        "        dataset).get_next()\n",
        "\n",
        "    # Extract features and label from the transformed tensors.\n",
        "    # TODO(b/30367437): make transformed_labels a dict.\n",
        "    transformed_labels = transformed_features.pop(LABEL_KEY)\n",
        "\n",
        "    return transformed_features, transformed_labels\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def _make_serving_input_fn(tf_transform_output):\n",
        "  \"\"\"Creates an input function reading from raw data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "\n",
        "  Returns:\n",
        "    The serving input function.\n",
        "  \"\"\"\n",
        "  raw_feature_spec = RAW_DATA_FEATURE_SPEC_PH.copy()\n",
        "  # Remove label since it is not available during serving.\n",
        "  raw_feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  def serving_input_fn():\n",
        "    \"\"\"Input function for serving.\"\"\"\n",
        "\n",
        "    raw_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(\n",
        "        raw_feature_spec, default_batch_size=None)\n",
        "    serving_input_receiver = raw_input_fn()\n",
        "    raw_features = serving_input_receiver.features\n",
        "    transformed_features = tf_transform_output.transform_raw_features(\n",
        "        raw_features)\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(\n",
        "        transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "  return serving_input_fn\n",
        "\n",
        "\n",
        "def get_feature_columns(tf_transform_output):\n",
        "  \"\"\"Returns the FeatureColumns for the model.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: A `TFTransformOutput` object.\n",
        "\n",
        "  Returns:\n",
        "    A list of FeatureColumns.\n",
        "  \"\"\"\n",
        "  # Wrap scalars as real valued columns.\n",
        "  real_valued_columns = [tf.feature_column.numeric_column(key, shape=())\n",
        "                         for key in NUMERIC_FEATURE_KEYS]\n",
        "\n",
        "  # Wrap categorical columns.\n",
        "  one_hot_columns = [\n",
        "      tf.feature_column.indicator_column(  # pylint: disable=g-complex-comprehension\n",
        "          tf.feature_column.categorical_column_with_vocabulary_file(\n",
        "              key=key,\n",
        "              vocabulary_file=tf_transform_output.vocabulary_file_by_name(\n",
        "                  vocab_filename=key)))\n",
        "      for key in CATEGORICAL_FEATURE_KEYS]\n",
        "\n",
        "  return real_valued_columns + one_hot_columns\n",
        "\n",
        "\n",
        "def train_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,\n",
        "                       num_test_instances=NUM_TEST_INSTANCES):\n",
        "  \"\"\"Train the model on training data and evaluate on test data.\n",
        "\n",
        "  Args:\n",
        "    working_dir: Directory to read transformed data and metadata from and to\n",
        "        write exported model to.\n",
        "    num_train_instances: Number of instances in train set\n",
        "    num_test_instances: Number of instances in test set\n",
        "\n",
        "  Returns:\n",
        "    The results from the estimator's 'evaluate' method\n",
        "  \"\"\"\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  run_config = tf.estimator.RunConfig()\n",
        "\n",
        "  estimator = tf.estimator.LinearClassifier(\n",
        "      feature_columns=get_feature_columns(tf_transform_output),\n",
        "      config=run_config,\n",
        "      loss_reduction=tf.losses.Reduction.SUM)\n",
        "\n",
        "  # Fit the model using the default optimizer.\n",
        "  train_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
        "      batch_size=TRAIN_BATCH_SIZE)\n",
        "  estimator.train(\n",
        "      input_fn=train_input_fn,\n",
        "      max_steps=TRAIN_NUM_EPOCHS * num_train_instances / TRAIN_BATCH_SIZE)\n",
        "\n",
        "  # Evaluate model on test dataset.\n",
        "  eval_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n",
        "      batch_size=1)\n",
        "\n",
        "  # Export the model.\n",
        "  serving_input_fn = _make_serving_input_fn(tf_transform_output)\n",
        "  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
        "  estimator.export_saved_model(exported_model_dir, serving_input_fn)\n",
        "\n",
        "  return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)\n",
        "\n",
        "\n",
        "input_data_dir = \".\"\n",
        "working_dir = \"census_model\" # tempfile.mkdtemp(dir=input_data_dir)\n",
        "train_data_file = os.path.join(input_data_dir, 'adult.data')\n",
        "test_data_file = os.path.join(input_data_dir, 'adult.test')\n",
        "\n",
        "transform_data(train_data_file, test_data_file, working_dir)\n",
        "\n",
        "results = train_and_evaluate(working_dir)\n",
        "\n",
        "pprint.pprint(results)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHmtNlmMgYgR",
        "colab_type": "text"
      },
      "source": [
        "## Authenticate if running from colab (and not in AI Platform notebooks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRQT-Zwjd8yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gcloud auth login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JngiiljyeCqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gcloud config set project <your project id>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4fHVwoeZSP4",
        "colab_type": "text"
      },
      "source": [
        "## Set your project, bucket, and model info here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k9T5e1DL2qE8",
        "colab": {}
      },
      "source": [
        "%env LOCAL_MODEL_DIR=./census_model/exported_model_dir\n",
        "%env PROJECT_ID=<your project-id>\n",
        "%env MODEL_GCS_DIR=census_model\n",
        "%env VERSION_NAME=v1\n",
        "%env MODEL_NAME=census\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNdNs6wAghRK",
        "colab_type": "text"
      },
      "source": [
        "## Get the saved model dir (most recent by timestamp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7He0hl7AMEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "mypath=os.getenv(\"LOCAL_MODEL_DIR\")\n",
        "onlyfiles = [f for f in os.listdir(mypath) if not os.path.isfile(os.path.join(mypath, f))]\n",
        "SAVED_MODEL_DIR=onlyfiles[0]\n",
        "SAVED_MODEL_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TepKiirLzgSL",
        "colab": {}
      },
      "source": [
        "%%bash -s \"$SAVED_MODEL_DIR\"\n",
        "gsutil cp -r ${LOCAL_MODEL_DIR}/$1 gs://${PROJECT_ID}/${MODEL_GCS_DIR}/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "imFaTxCpONbz",
        "colab": {}
      },
      "source": [
        "# set this if you need to use the python client; upload a service account json file\n",
        "# %env GOOGLE_APPLICATION_CREDENTIALS=<path to your service account json file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J26rAFdGJhjJ",
        "colab": {}
      },
      "source": [
        "%%bash -s \"$SAVED_MODEL_DIR\"\n",
        "# use this to inspect the model metadata signature; specify a local or GCS path\n",
        "# saved_model_cli show --dir gs://${PROJECT_ID}/${MODEL_GCS_DIR}/$1 --all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVQzYml4grP3",
        "colab_type": "text"
      },
      "source": [
        "## Create the model and version in AI Platform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FbOOm9NK2Wx4",
        "colab": {}
      },
      "source": [
        "!gcloud ai-platform models create ${MODEL_NAME}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K3CNk9Z9iwtb",
        "colab": {}
      },
      "source": [
        "# run this if you need to delete and recreate the model\n",
        "#!gcloud ai-platform versions delete $VERSION_NAME --model=$MODEL_NAME --region=$REGION --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u-tXWVD126Cq",
        "colab": {}
      },
      "source": [
        "%%bash -s \"$SAVED_MODEL_DIR\"\n",
        "gcloud ai-platform versions create $VERSION_NAME \\\n",
        "  --model=$MODEL_NAME \\\n",
        "  --origin=gs://${PROJECT_ID}/${MODEL_GCS_DIR}/$1 \\\n",
        "  --runtime-version=1.15 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU6Vv-lggxNW",
        "colab_type": "text"
      },
      "source": [
        "## Python API client"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IIr24W-vNsVf",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import base64\n",
        "import json\n",
        "\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from googleapiclient import discovery\n",
        "import six\n",
        "\n",
        "def predict_json(project, model, instances, version='v1'):\n",
        "    \"\"\"Send json data to a deployed model for prediction.\n",
        "\n",
        "    Args:\n",
        "        project (str): project where the AI Platform Model is deployed.\n",
        "        model (str): model name.\n",
        "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
        "            your deployed model expects as inputs. Values should be datatypes\n",
        "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
        "            convertible to tensors.\n",
        "        version: str, version of the model to target.\n",
        "    Returns:\n",
        "        Mapping[str: any]: dictionary of prediction results defined by the\n",
        "            model.\n",
        "    \"\"\"\n",
        "    # Create the AI Platform service object.\n",
        "    # To authenticate, make sure the envirnment variable is set somewhere above \n",
        "    # GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_file>\n",
        "\n",
        "\n",
        "    ml = discovery.build('ml', 'v1')\n",
        "    name = 'projects/{}/models/{}'.format(project, model)\n",
        "\n",
        "    if version is not None:\n",
        "        name += '/versions/{}'.format(version)\n",
        "    request_body = { 'instances': [instances] }\n",
        "    request = ml.projects().predict(\n",
        "        name=name,\n",
        "        body=request_body)\n",
        "\n",
        "    response = request.execute()\n",
        "\n",
        "    if 'error' in response:\n",
        "        raise RuntimeError(response['error'])\n",
        "\n",
        "    return response['predictions']\n",
        "\n",
        "predict_json(os.getenv('PROJECT_ID'), os.getenv('MODEL_NAME'),  \n",
        "             {\"age\":50.0, \"workclass\":\"Self-emp-not-inc\", \"education\":\"Bachelors\", \"education_num\":13.0, \n",
        "              \"marital_status\":\"Married-civ-spouse\", \"occupation\":\"Exec-managerial\", \"relationship\":\"Husband\", \n",
        "              \"race\":\"White\", \"sex\":\"Male\", \"capital_gain\":0.0, \"capital_loss\":0.0, \"hours_per_week\":13.0, \"native_country\":\"United-States\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIOhzV7Sg_CI",
        "colab_type": "text"
      },
      "source": [
        "## Test via gcloud CLI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ehho7n87Gq0p",
        "colab": {}
      },
      "source": [
        "%%writefile test.json\n",
        "{\"age\":50.0, \"workclass\":\"Self-emp-not-inc\", \"education\":\"Bachelors\", \"education_num\":13.0, \"marital_status\":\"Married-civ-spouse\", \"occupation\":\"Exec-managerial\", \"relationship\":\"Husband\", \"race\":\"White\", \"sex\":\"Male\", \"capital_gain\":0.0, \"capital_loss\":0.0, \"hours_per_week\":13.0, \"native_country\":\"United-States\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YBsDCz_0BI7a",
        "colab": {}
      },
      "source": [
        "!gcloud ai-platform predict --model ${MODEL_NAME} --version v1 --json-instances test.json\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}